{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a03f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "archivo = 'dataset_2017_full_clean.csv'\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(archivo)\n",
    "\n",
    "# Eliminar las filas que contengan twhashtag, twmention de la columna 'tweet'\n",
    "\n",
    "df = df[~df['tweet'].str.contains('twhashtag|twmention', na=False)]\n",
    "\n",
    "# Eliminar las comillas de las filas\n",
    "df['tweet'] = df['tweet'].str.replace('\"', '', regex=False)\n",
    "\n",
    "# Eliminar los emojis de las filas\n",
    "df['tweet'] = df['tweet'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "\n",
    "# Eliminar lo que machee con esto @\\S+ \n",
    "df['tweet'] = df['tweet'].str.replace(r'@\\S+ ', '', regex=True)\n",
    "\n",
    "# Guardar el dataset limpio\n",
    "df.to_csv('comentarios_dataset_limpitov1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e7cba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesamiento completado.\n",
      "  X_train_padded.npy → (17577, 100)\n",
      "  X_test_padded.npy  → (4395, 100)\n",
      "  y_train.npy        → (17577, 3)\n",
      "  y_test.npy         → (4395, 3)\n",
      "Mapas guardados: word2idx.json, label_mapping.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ruta al CSV\n",
    "CSV_PATH = 'comentarios_dataset_limpitov1.csv'\n",
    "\n",
    "# Parámetros\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Lista simple de stopwords en español\n",
    "STOPWORDS = {\n",
    "    'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se',\n",
    "    'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al',\n",
    "    'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este',\n",
    "    'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre',\n",
    "    'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo',\n",
    "    'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros',\n",
    "    'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos',\n",
    "    'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa',\n",
    "    'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella',\n",
    "    'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú',\n",
    "    'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosostros', 'vosostras',\n",
    "    'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas',\n",
    "    'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros',\n",
    "    'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos',\n",
    "    'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están',\n",
    "    'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás',\n",
    "    'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías',\n",
    "    'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos',\n",
    "    'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos',\n",
    "    'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos',\n",
    "    'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos',\n",
    "    'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados',\n",
    "    'estadas', 'estad'\n",
    "}\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    # 1) pasar a minúsculas\n",
    "    texto = str(texto).lower()\n",
    "    # 2) eliminar URLs\n",
    "    texto = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto)\n",
    "    # 3) eliminar caracteres que no sean letras, números o espacios\n",
    "    texto = re.sub(r'[^a-záéíóúüñ0-9\\s]', '', texto)\n",
    "    # 4) normalizar espacios\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    # 5) remover stopwords\n",
    "    tokens = texto.split()\n",
    "    tokens_filtrados = [tok for tok in tokens if tok not in STOPWORDS]\n",
    "    return ' '.join(tokens_filtrados)\n",
    "\n",
    "def construir_vocabulario(textos, max_size):\n",
    "    contador = Counter()\n",
    "    for t in textos:\n",
    "        tokens = t.split()\n",
    "        contador.update(tokens)\n",
    "    mas_comunes = contador.most_common(max_size - 2)  # Reservar índices 0 y 1\n",
    "    word2idx = {palabra: idx + 2 for idx, (palabra, _) in enumerate(mas_comunes)}\n",
    "    word2idx['<OOV>'] = 1\n",
    "    return word2idx\n",
    "\n",
    "def texto_a_secuencia(texto, word2idx):\n",
    "    return [word2idx.get(tok, 1) for tok in texto.split()]\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "    else:\n",
    "        return seq[:max_len]\n",
    "\n",
    "def main():\n",
    "    # 1) Cargar CSV\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    # 2) Eliminar filas con sentiment = -1 (NONE)\n",
    "    df = df[df['sentiment'] != -1]\n",
    "    \n",
    "    # 3) Limpiar textos y eliminar vacíos\n",
    "    df['clean_tweet'] = df['tweet'].apply(limpiar_texto)\n",
    "    df = df[df['clean_tweet'] != '']\n",
    "    \n",
    "    # 4) Codificar etiquetas\n",
    "    #    0 = Neutral, 1 = Positivo, 2 = Negativo\n",
    "    clases = sorted(df['sentiment'].unique())  # [0, 1, 2]\n",
    "    mapeo_labels = {label: idx for idx, label in enumerate(clases)}\n",
    "    df['label_idx'] = df['sentiment'].map(mapeo_labels)\n",
    "    num_clases = len(clases)\n",
    "    y = np.eye(num_clases)[df['label_idx']]\n",
    "    \n",
    "    # 5) Split train/test estratificado\n",
    "    X = df['clean_tweet'].values\n",
    "    y_labels = df['label_idx'].values\n",
    "    X_train, X_test, y_train_idx, y_test_idx = train_test_split(\n",
    "        X, y_labels,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_labels\n",
    "    )\n",
    "    y_train = np.eye(num_clases)[y_train_idx]\n",
    "    y_test = np.eye(num_clases)[y_test_idx]\n",
    "    \n",
    "    # 6) Construir vocabulario solo con entrenamiento\n",
    "    word2idx = construir_vocabulario(X_train, MAX_VOCAB_SIZE)\n",
    "    \n",
    "    # 7) Convertir textos a secuencias de índices\n",
    "    seq_train = [texto_a_secuencia(t, word2idx) for t in X_train]\n",
    "    seq_test = [texto_a_secuencia(t, word2idx) for t in X_test]\n",
    "    \n",
    "    # 8) Padding y truncamiento\n",
    "    X_train_padded = np.array([pad_sequence(s, MAX_SEQUENCE_LENGTH) for s in seq_train], dtype=np.int32)\n",
    "    X_test_padded = np.array([pad_sequence(s, MAX_SEQUENCE_LENGTH) for s in seq_test], dtype=np.int32)\n",
    "    \n",
    "    # 9) Guardar vectores y mapeos\n",
    "    np.save('X_train_padded.npy', X_train_padded)\n",
    "    np.save('X_test_padded.npy', X_test_padded)\n",
    "    np.save('y_train.npy', y_train)\n",
    "    np.save('y_test.npy', y_test)\n",
    "    \n",
    "    with open('word2idx.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(word2idx, f, ensure_ascii=False, indent=2)\n",
    "    mapeo_labels_str = {str(k): int(v) for k, v in mapeo_labels.items()}\n",
    "    with open('label_mapping.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(mapeo_labels_str, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"Preprocesamiento completado.\")\n",
    "    print(f\"  X_train_padded.npy → {X_train_padded.shape}\")\n",
    "    print(f\"  X_test_padded.npy  → {X_test_padded.shape}\")\n",
    "    print(f\"  y_train.npy        → {y_train.shape}\")\n",
    "    print(f\"  y_test.npy         → {y_test.shape}\")\n",
    "    print(\"Mapas guardados: word2idx.json, label_mapping.json\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
